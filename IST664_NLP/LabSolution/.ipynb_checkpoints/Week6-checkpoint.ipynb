{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab Week 5 - Lexical Semantics:  WordNet\n",
    "# This file has small examples that are meant to be run individually\n",
    "#   in the Python shell\n",
    "\n",
    "import nltk\n",
    "\n",
    "# import wordnet and shorten its name to wn\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('maple.n.01'), Synset('maple.n.02')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('maple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('wood.n.01')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find hypernyms of synsets\n",
    "map1 = wn.synset('maple.n.01')\n",
    "map1.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the most general hypernym of a synset\n",
    "map1.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('matter.n.03'),\n",
       " Synset('substance.n.01'),\n",
       " Synset('material.n.01'),\n",
       " Synset('plant_material.n.01'),\n",
       " Synset('wood.n.01'),\n",
       " Synset('maple.n.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of paths from the synset to the root concept \"entity\"\n",
    "paths=map1.hypernym_paths()\n",
    "print(len(paths) )\n",
    "# look at the first path\n",
    "paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the wordnet browser, we see that dog1 has two more relations\n",
    "map1.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the wordnet browser, we see that dog1 has two more relations\n",
    "map1.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/erm1000255241/nltk_data'\n    - '/Users/erm1000255241/anaconda3/nltk_data'\n    - '/Users/erm1000255241/anaconda3/share/nltk_data'\n    - '/Users/erm1000255241/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/erm1000255241/nltk_data'\n    - '/Users/erm1000255241/anaconda3/nltk_data'\n    - '/Users/erm1000255241/anaconda3/share/nltk_data'\n    - '/Users/erm1000255241/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-be6e10e323ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#    whether it is noun, verb, adj or adverb and a number among the synsets of that word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# given word \"dog\", returns the ids of the synsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# given a synset id, find words/lemma names (the synonyms) of the first noun sense of \"dog\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/erm1000255241/nltk_data'\n    - '/Users/erm1000255241/anaconda3/nltk_data'\n    - '/Users/erm1000255241/anaconda3/share/nltk_data'\n    - '/Users/erm1000255241/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# for each sense of a word, there is a synset with an id consisting of one of the words,\n",
    "#    whether it is noun, verb, adj or adverb and a number among the synsets of that word\n",
    "# given word \"dog\", returns the ids of the synsets\n",
    "wn.synsets('dog')\n",
    "\n",
    "# given a synset id, find words/lemma names (the synonyms) of the first noun sense of \"dog\"\n",
    "wn.synset('dog.n.01').lemma_names()\n",
    "\n",
    "# given a synset id, find lemmas of the synset (a lemma pairs a word with a synset)\n",
    "wn.synset('dog.n.01').lemmas()\n",
    "\n",
    "# find synset of a lemma\n",
    "wn.lemma('dog.n.01.domestic_dog').synset()\n",
    "\n",
    "# find lemma names for all senses of a word\n",
    "for synset in wn.synsets('dog'):\n",
    "\tprint (synset, \":  \", synset.lemma_names())\n",
    "\n",
    "# find definition of the first noun sense of dog, or namely, the dog.n.01 synset\n",
    "wn.synset('dog.n.01').definition()\n",
    "\n",
    "# display an example of the synset\n",
    "wn.synset('dog.n.01').examples()\n",
    "\n",
    "# or show the definitions for all the synsets of a word\n",
    "for synset in wn.synsets('dog'):\n",
    "\tprint (synset, \":  \", synset.definition())\n",
    "\n",
    "# or combine the synonyms/lemma names, definitions and examples\n",
    "for synset in wn.synsets('dog'):\n",
    "\tprint (synset, \":  \")\n",
    "\tprint ('     ', synset.lemma_names())\n",
    "\tprint ('     ', synset.definition())\n",
    "\tprint ('     ', synset.examples())\n",
    "\n",
    "\n",
    "##  Lexical relations between synsets in WordNet\n",
    "# find hypernyms of synsets\n",
    "dog1 = wn.synset('dog.n.01')\n",
    "dog1.hypernyms()\n",
    "\n",
    "# find hyponyms\n",
    "dog1.hyponyms()\n",
    "\n",
    "# the most general hypernym of a synset\n",
    "dog1.root_hypernyms()\n",
    "\n",
    "# from the wordnet browser, we see that dog1 has two more relations\n",
    "dog1.part_meronyms()\n",
    "\n",
    "# what is this?  check it out \n",
    "print (wn.synset('flag.n.07').lemma_names(),wn.synset('flag.n.07').definition(), \n",
    "       wn.synset('flag.n.07').examples())\n",
    "\n",
    "# the other relation for dog1\n",
    "dog1.member_holonyms()\n",
    "\n",
    "\n",
    "# look at another word, the adjective \"good\"\n",
    "wn.synsets('good')\n",
    "\n",
    "# find antonyms, sometimes need to specify for which lemma the antonym is needed\n",
    "good1 = wn.synset('good.a.01')\n",
    "# display synonyms of this synset\n",
    "good1.lemma_names()\n",
    "\n",
    "# the antonym function is defined only on the lemma, not the synset\n",
    "# find antonym for the first lemma of the synset\n",
    "print(good1.lemmas())\n",
    "good1.lemmas()[0].antonyms() \n",
    "\n",
    "\n",
    "# find entailments of verbs\n",
    "print(wn.synset('walk.v.01').entailments())\n",
    "print(wn.synset('eat.v.01').entailments())\n",
    "\n",
    "\n",
    "# trace paths of a synset by visiting its hypernyms\n",
    "dog1.hypernyms()\n",
    "\n",
    "# number of paths from the synset to the root concept \"entity\"\n",
    "paths=dog1.hypernym_paths()\n",
    "print(len(paths) )\n",
    "# look at the first path\n",
    "paths[0]\n",
    "\n",
    "# or just list the names in the paths\n",
    "#list the first path\n",
    "[synset.name() for synset in paths[0]]\n",
    "\n",
    "#list the second path \n",
    "[synset.name() for synset in paths[1]] \n",
    "\n",
    "\n",
    "### Word similarity\n",
    "\n",
    "# define 3 different types of whales\n",
    "right = wn.synset('right_whale.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')  \n",
    "orca = wn.synset('orca.n.01') \n",
    "\n",
    "# look at the paths of these three whales\n",
    "print(right.hypernym_paths())\n",
    "print(minke.hypernym_paths())\n",
    "print(orca.hypernym_paths())\n",
    "\n",
    "# find the least ancestor of right and minke, and then right and orca\n",
    "print(right.lowest_common_hypernyms(minke))\n",
    "print(right.lowest_common_hypernyms(orca))\n",
    "\n",
    "# the function min_depth gives the length of a path from a word to the top of the hierarchy\n",
    "print(right.min_depth() )\n",
    "print(wn.synset('baleen_whale.n.01').min_depth() )\n",
    "print(wn.synset('entity.n.01').min_depth())\n",
    "\n",
    "# the path similarity gives a similarity score between 0 and 1\n",
    "print(right.path_similarity(minke) )\n",
    "print(right.path_similarity(orca))\n",
    "\n",
    "# define 2 more words and look at their similarity\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')\n",
    "# note the least ancestor of these two words\n",
    "print(right.lowest_common_hypernyms(tortoise))\n",
    "print(right.lowest_common_hypernyms(novel))\n",
    "\n",
    "print(right.path_similarity(tortoise) )\n",
    "print(right.path_similarity(novel))\n",
    "\n",
    "\n",
    "# other similarity measures\n",
    "help(wn)\n",
    "\n",
    "# Leacock-Chodorow Similarity, also uses path lengths and others\n",
    "print(right.lch_similarity(orca))\n",
    "print(right.lch_similarity(tortoise))\n",
    "print(right.lch_similarity(novel))\n",
    "\n",
    "# Resnick similarity gets information content measure\n",
    "# first get information content from a general corpus\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "# try Resnik Similarity\n",
    "print(right.res_similarity(orca, brown_ic))\n",
    "print(right.res_similarity(tortoise, brown_ic))\n",
    "print(right.res_similarity(novel, brown_ic))\n",
    "\n",
    "\n",
    "## SentiWordNet\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "# each word judged to be made up of positive, negative and objective meaning\n",
    "\n",
    "# sentiwordnet has the same synsets as wordnet, use wn functions\n",
    "print(list(swn.senti_synsets('breakdown')))\n",
    "print(wn.synsets('breakdown'))\n",
    "\n",
    "\n",
    "# the print function gives the positive and negative scores\n",
    "breakdown3 = swn.senti_synset('breakdown.n.03')\n",
    "print (breakdown3)\n",
    "\n",
    "\n",
    "# there are also separate functions for all the scores\n",
    "print(breakdown3.pos_score())\n",
    "print(breakdown3.neg_score())\n",
    "print(breakdown3.obj_score())\n",
    "\n",
    "\n",
    "# some more exploration of sentiment scores of words\n",
    "dogswn1 = swn.senti_synset('dog.n.01')\n",
    "print(dogswn1)\n",
    "print(dogswn1.obj_score())\n",
    "\n",
    "\n",
    "goodswn1 = swn.senti_synset('good.a.01')\n",
    "print(goodswn1)\n",
    "print(goodswn1.obj_score())\n",
    "\n",
    "\n",
    "# not all words in WordNet have been scored for sentiment in SentiWordNet\n",
    "#   but the most recent version has scored a lot more so I don't have an example right now\n",
    "print(wn.synsets('exuberant'))\n",
    "ex3 = swn.senti_synset('exuberant.s.03')\n",
    "print(ex3)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SentiSynset('maple.n.01'), SentiSynset('maple.n.02')]\n",
      "[Synset('maple.n.01'), Synset('maple.n.02')]\n",
      "<maple.n.02: PosScore=0.0 NegScore=0.0>\n",
      "0.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# NLP Lab Session Week 6\n",
    "# WordNet in NLTK\n",
    "# Part 4:  SentiWordNet\n",
    "\n",
    "# SentiWordNet, a sentiment lexicon in NLTK\n",
    "\n",
    "# So far, there appears to be only one sentiment lexicon in NLTK.  [You may want to review the lists of NLTK resources http://www.nltk.org/howto/corpus.html  ]  The documentation for SentiWordNet is on this HowTo page:  http://www.nltk.org/howto/sentiwordnet.html\n",
    "\n",
    "# In this sentiment lexicon, each word is judged to be made up of partly positive, negative and objective meaning, and 3 scores are given as to how much of each, where the scores must sum to 1.\n",
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Note that there are not very many functions listed, but the trick is that the synsets in SentiWordNet are the same as in WordNet, so we can use the functions to find the synonyms, definitions and examples in wordnet.\n",
    "\n",
    "# sentiwordnet has the same synsets as wordnet, use wn functions\n",
    "print(list(swn.senti_synsets('maple')))\n",
    "print(wn.synsets('maple'))\n",
    "\n",
    "# Following the example in the HowTo page, we look at the third sense of the word breakdown:\n",
    "\n",
    "map2 = swn.senti_synset('maple.n.02')\n",
    "\n",
    "# The print function for senti_synsets will give the positive and negative scores.\n",
    "print (map2)\n",
    "\n",
    "# There are functions to access these two scores separately, and a third function to access the objective scores.\n",
    "\n",
    "print(map2.pos_score())\n",
    "print(map2.neg_score())\n",
    "print(map2.obj_score())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
