{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = '/Users/erm1000255241/Library/Mobile Documents/com~apple~CloudDocs/Documents/SyracuseUniversity/5th_Quarter/IST664 /LabWeek8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Subjectivity reads the subjectivity lexicon file from Wiebe et al\n",
    "#    at http://www.cs.pitt.edu/mpqa/ (part of the Multiple Perspective QA project)\n",
    "#\n",
    "# This file has the format that each line is formatted as in this example for the word \"abandoned\"\n",
    "#     type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
    "# In our data, the pos tag is ignored, so this program just takes the last one read\n",
    "#     (typically the noun over the adjective)\n",
    "#\n",
    "# The data structure that is created is a dictionary where\n",
    "#    each word is mapped to a list of 4 things:  \n",
    "#        strength, which will be either 'strongsubj' or 'weaksubj'\n",
    "#        posTag, either 'adj', 'verb', 'noun', 'adverb', 'anypos'\n",
    "#        isStemmed, either true or false\n",
    "#        polarity, either 'positive', 'negative', or 'neutral'\n",
    "\n",
    "import nltk\n",
    "\n",
    "# pass the absolute path of the lexicon file to this program\n",
    "# example call:\n",
    "# nancymacpath = \n",
    "#    \"/Users/njmccrac/AAAdocs/research/subjectivitylexicon/hltemnlp05clues/subjclueslen1-HLTEMNLP05.tff\"\n",
    "# SL = readSubjectivity(nancymacpath)\n",
    "\n",
    "# this function returns a dictionary where you can look up words and get back \n",
    "#     the four items of subjectivity information described above\n",
    "def readSubjectivity(path):\n",
    "    path = path_folder + path\n",
    "    flexicon = open(path, 'r')\n",
    "    # initialize an empty dictionary\n",
    "    sldict = { }\n",
    "    for line in flexicon:\n",
    "        fields = line.split()   # default is to split on whitespace\n",
    "        # split each field on the '=' and keep the second part as the value\n",
    "        strength = fields[0].split(\"=\")[1]\n",
    "        word = fields[2].split(\"=\")[1]\n",
    "        posTag = fields[3].split(\"=\")[1]\n",
    "        stemmed = fields[4].split(\"=\")[1]\n",
    "        polarity = fields[5].split(\"=\")[1]\n",
    "        if (stemmed == 'y'):\n",
    "            isStemmed = True\n",
    "        else:\n",
    "            isStemmed = False\n",
    "        # put a dictionary entry with the word as the keyword\n",
    "        #     and a list of the other values\n",
    "        sldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "    return sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "['neg', 'pos']\n",
      "['simplistic', ',', 'silly', 'and', 'tedious', '.']\n",
      "[\"it's\", 'so', 'laddish', 'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find', 'it', 'funny', '.']\n",
      "['exploitative', 'and', 'largely', 'devoid', 'of', 'the', 'depth', 'or', 'sophistication', 'that', 'would', 'make', 'watching', 'such', 'a', 'graphic', 'treatment', 'of', 'the', 'crimes', 'bearable', '.']\n",
      "['[garbus]', 'discards', 'the', 'potential', 'for', 'pathological', 'study', ',', 'exhuming', 'instead', ',', 'the', 'skewed', 'melodrama', 'of', 'the', 'circumstantial', 'situation', '.']\n",
      "5331\n",
      "5331\n",
      "(['simplistic', ',', 'silly', 'and', 'tedious', '.'], 'neg')\n",
      "(['provides', 'a', 'porthole', 'into', 'that', 'noble', ',', 'trembling', 'incoherence', 'that', 'defines', 'us', 'all', '.'], 'pos')\n",
      "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'that', 'it', 'as', 'but', 'with', 'film', 'this', 'for', 'its', 'an', 'movie', \"it's\", 'be', 'on', 'you', 'not', 'by', 'about', 'one', 'more', 'like', 'has', 'are', 'at', 'from', 'than', '\"', 'all', '--', 'his', 'have', 'so', 'if', 'or', 'story', 'i', 'too', 'just', 'who', 'into', 'what']\n",
      "Most Informative Features\n",
      "             V_wonderful = True              pos : neg    =     21.6 : 1.0\n",
      "            V_engrossing = True              pos : neg    =     20.9 : 1.0\n",
      "                  V_dull = True              neg : pos    =     15.5 : 1.0\n",
      "               V_routine = True              neg : pos    =     15.0 : 1.0\n",
      "               V_generic = True              neg : pos    =     15.0 : 1.0\n",
      "             V_inventive = True              pos : neg    =     14.3 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "                V_boring = True              neg : pos    =     13.6 : 1.0\n",
      "                  V_flat = True              neg : pos    =     13.4 : 1.0\n",
      "             V_absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "                  V_warm = True              pos : neg    =     11.8 : 1.0\n",
      "                    V_90 = True              neg : pos    =     11.7 : 1.0\n",
      "              V_mindless = True              neg : pos    =     11.0 : 1.0\n",
      "              V_powerful = True              pos : neg    =     11.0 : 1.0\n",
      "               V_quietly = True              pos : neg    =     11.0 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     11.0 : 1.0\n",
      "                 V_vivid = True              pos : neg    =     11.0 : 1.0\n",
      "              V_provides = True              pos : neg    =     10.6 : 1.0\n",
      "                 V_stale = True              neg : pos    =     10.4 : 1.0\n",
      "                V_deftly = True              pos : neg    =     10.3 : 1.0\n",
      "              V_chilling = True              pos : neg    =     10.3 : 1.0\n",
      "              V_captures = True              pos : neg    =     10.2 : 1.0\n",
      "                    V_tv = True              neg : pos    =     10.2 : 1.0\n",
      "                 V_flaws = True              pos : neg    =      9.8 : 1.0\n",
      "                V_beauty = True              pos : neg    =      9.8 : 1.0\n",
      "            V_meandering = True              neg : pos    =      9.7 : 1.0\n",
      "            V_apparently = True              neg : pos    =      9.7 : 1.0\n",
      "                  V_ages = True              pos : neg    =      9.6 : 1.0\n",
      "             V_realistic = True              pos : neg    =      9.6 : 1.0\n",
      "['strongsubj', 'adj', False, 'neutral']\n",
      "['strongsubj', 'adj', False, 'negative']\n",
      "neutral\n",
      "15\n",
      "3\n",
      "['there', 'is', 'a', 'difference', 'between', 'movies', 'with', 'the', 'courage', 'to', 'go', 'over', 'the', 'top', 'and', 'movies', 'that', \"don't\", 'care', 'about', 'being', 'stupid']\n",
      "['a', 'farce', 'of', 'a', 'parody', 'of', 'a', 'comedy', 'of', 'a', 'premise', ',', 'it', \"isn't\", 'a', 'comparison', 'to', 'reality', 'so', 'much', 'as', 'it', 'is', 'a', 'commentary', 'about', 'our', 'knowledge', 'of', 'films', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['most', 'of', 'the', 'problems', 'with', 'the', 'film', \"don't\", 'derive', 'from', 'the', 'screenplay', ',', 'but', 'rather', 'the', 'mediocre', 'performances', 'by', 'most', 'of', 'the', 'actors', 'involved']\n",
      "['the', 'lack', 'of', 'naturalness', 'makes', 'everything', 'seem', 'self-consciously', 'poetic', 'and', 'forced', '.', '.', '.', \"it's\", 'a', 'pity', 'that', \"[nelson's]\", 'achievement', \"doesn't\", 'match', 'his', 'ambition', '.']\n",
      "False\n",
      "False\n",
      "Most Informative Features\n",
      "             V_wonderful = True              pos : neg    =     21.6 : 1.0\n",
      "            V_engrossing = True              pos : neg    =     20.9 : 1.0\n",
      "                  V_dull = True              neg : pos    =     15.5 : 1.0\n",
      "               V_routine = True              neg : pos    =     15.0 : 1.0\n",
      "               V_generic = True              neg : pos    =     15.0 : 1.0\n",
      "             V_inventive = True              pos : neg    =     14.3 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "                V_boring = True              neg : pos    =     13.6 : 1.0\n",
      "                  V_flat = True              neg : pos    =     13.4 : 1.0\n",
      "             V_absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "                  V_warm = True              pos : neg    =     11.8 : 1.0\n",
      "                    V_90 = True              neg : pos    =     11.7 : 1.0\n",
      "              V_mindless = True              neg : pos    =     11.0 : 1.0\n",
      "              V_powerful = True              pos : neg    =     11.0 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     11.0 : 1.0\n",
      "                 V_vivid = True              pos : neg    =     11.0 : 1.0\n",
      "               V_quietly = True              pos : neg    =     11.0 : 1.0\n",
      "              V_provides = True              pos : neg    =     10.6 : 1.0\n",
      "                 V_stale = True              neg : pos    =     10.4 : 1.0\n",
      "              V_chilling = True              pos : neg    =     10.3 : 1.0\n",
      "                V_deftly = True              pos : neg    =     10.3 : 1.0\n",
      "              V_captures = True              pos : neg    =     10.2 : 1.0\n",
      "                    V_tv = True              neg : pos    =     10.2 : 1.0\n",
      "                V_beauty = True              pos : neg    =      9.8 : 1.0\n",
      "                 V_flaws = True              pos : neg    =      9.8 : 1.0\n",
      "            V_apparently = True              neg : pos    =      9.7 : 1.0\n",
      "            V_meandering = True              neg : pos    =      9.7 : 1.0\n",
      "                  V_ages = True              pos : neg    =      9.6 : 1.0\n",
      "             V_realistic = True              pos : neg    =      9.6 : 1.0\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "157\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\"]\n",
      "['.', ',', 'film', 'movie', 'not', 'one', 'like', '\"', '--', 'story', 'no', 'much', 'even', 'good', 'comedy', 'time', 'characters', 'little', 'way', 'funny', 'make', 'enough', 'never', 'makes', 'may', 'us', 'work', 'best', 'bad', 'director']\n"
     ]
    }
   ],
   "source": [
    "## Lab Week 8 - Sentiment Classification on Sentences from the Movie Reviews\n",
    "# This file has small examples that are meant to be run individually\n",
    "#   in the Python shell\n",
    "\n",
    "import nltk\n",
    "\n",
    "# movie review sentences\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "\n",
    "# get the sentence corpus and look at some sentences\n",
    "sentences = sentence_polarity.sents()\n",
    "print(len(sentences))\n",
    "print(type(sentences))\n",
    "print(sentence_polarity.categories())\n",
    "# sentences are already tokenized, print the first four sentences\n",
    "for sent in sentences[:4]:\n",
    "    print(sent)\n",
    "\n",
    "# look at the sentences by category to see how many positive and negative\n",
    "pos_sents = sentence_polarity.sents(categories='pos')\n",
    "print(len(pos_sents))\n",
    "neg_sents = sentence_polarity.sents(categories='neg')\n",
    "print(len(neg_sents))\n",
    "\n",
    "## setup the movie reviews sentences for classification\n",
    "# create a list of documents, each document is one sentence as a list of words paired with category\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "\tfor sent in sentence_polarity.sents(categories=cat)]\n",
    "\n",
    "# look at the first and last documents - consists of all the words in the review\n",
    "# followed by the category\n",
    "print(documents[0])\n",
    "print(documents[-1])\n",
    "# randomly reorder documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "\n",
    "# get all words from all movie_reviews and put into a frequency distribution\n",
    "#   note lowercase, but no stemming or stopwords\n",
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "# get the 2000 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "print(word_features[:50])\n",
    "\n",
    "\n",
    "# define features (keywords) of a document for a BOW/unigram baseline\n",
    "# each feature is 'contains(keyword)' and is true or false depending\n",
    "# on whether that keyword is in the document\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# get features sets for a document, including keyword features and category feature\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]\n",
    "\n",
    "# the feature sets are 2000 words long so you may not want to look at one\n",
    "featuresets[0]\n",
    "\n",
    "# training using naive Baysian classifier, training set is approximately 90% of data\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# evaluate the accuracy of the classifier\n",
    "nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "# the accuracy result may vary since we randomized the documents\n",
    "\n",
    "# show which features of classifier are most informative\n",
    "classifier.show_most_informative_features(30)\n",
    "\n",
    "####   adding features   ####\n",
    "# First run the program in the file Subjectivity.py to load the subjectivity lexicon\n",
    "# copy and paste the definition of the readSubjectivity functions\n",
    "\n",
    "# create a path to where the subjectivity file resides on your disk\n",
    "# this example is for my mac\n",
    "# nancymacpath = \"/Users/njmccrac1/AAAdocs/research/subjectivitylexicon/hltemnlp05clues/subjclueslen1-HLTEMNLP05.tff\"\n",
    "\n",
    "# create your own path to the subjclues file\n",
    "SLpath = \"subjclueslen1-HLTEMNLP05.tff\"\n",
    "\n",
    "# import the Subjectivity program as a module to use the function\n",
    "SL = readSubjectivity(SLpath)\n",
    "\n",
    "# or copy the readSubjectivity function into your session and cal the fn\n",
    "SL = readSubjectivity(SLpath)\n",
    "\n",
    "# how many words are in the dictionary\n",
    "len(SL.keys())\n",
    "\n",
    "# look at words in the dictionary\n",
    "print(SL['absolute'])\n",
    "print(SL['shabby'])\n",
    "# note what happens if the word is not there\n",
    "# print(SL['dog'])\n",
    "\n",
    "# use multiple assignment to get the 4 items\n",
    "strength, posTag, isStemmed, polarity = SL['absolute']\n",
    "print(polarity)\n",
    "\n",
    "# define features that include word counts of subjectivity words\n",
    "# negative feature will have number of weakly negative words +\n",
    "#    2 * number of strongly negative words\n",
    "# positive feature has similar definition\n",
    "#    not counting neutral words\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features\n",
    "\n",
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in documents]\n",
    "\n",
    "# show just the two sentiment lexicon features in document 0\n",
    "print(SL_featuresets[0][0]['positivecount'])\n",
    "print(SL_featuresets[0][0]['negativecount'])\n",
    "\n",
    "# this gives the label of document 0\n",
    "SL_featuresets[0][1]\n",
    "# number of features for document 0\n",
    "len(SL_featuresets[0][0].keys())\n",
    "\n",
    "# retrain the classifier using these features\n",
    "train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "\n",
    "###  Negation words\n",
    "# Negation words \"not\", \"never\" and \"no\"\n",
    "# Not can appear in contractions of the form \"doesn't\"\n",
    "for sent in list(sentences)[:50]:\n",
    "    for word in sent:\n",
    "        if (word.endswith(\"n't\")):\n",
    "            print(sent)\n",
    "\n",
    "# this list of negation words includes some \"approximate negators\" like hardly and rarely\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "\n",
    "# One strategy with negation words is to negate the word following the negation word\n",
    "#   other strategies negate all words up to the next punctuation\n",
    "# Strategy is to go through the document words in order adding the word features,\n",
    "#   but if the word follows a negation words, change the feature to negated word\n",
    "# Start the feature set with all 2000 word features and 2000 Not word features set to false\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features\n",
    "\n",
    "# define the feature sets\n",
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]\n",
    "# show the values of a couple of example features\n",
    "print(NOT_featuresets[0][0]['V_NOTcare'])\n",
    "print(NOT_featuresets[0][0]['V_always'])\n",
    "\n",
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(30)\n",
    "\n",
    "\n",
    "### Bonus python text for the Question, define a stop word list ###\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(len(stopwords))\n",
    "print(stopwords)\n",
    "\n",
    "# remove some negation words \n",
    "negationwords.extend(['ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n",
    "\n",
    "newstopwords = [word for word in stopwords if word not in negationwords]\n",
    "print(len(newstopwords))\n",
    "print(newstopwords)\n",
    "\n",
    "# remove stop words from the all words list\n",
    "new_all_words_list = [word for (sent,cat) in documents for word in sent if word not in newstopwords]\n",
    "\n",
    "# continue to define a new all words dictionary, get the 2000 most common as new_word_features\n",
    "new_all_words = nltk.FreqDist(new_all_words_list)\n",
    "new_word_items = new_all_words.most_common(2000)\n",
    "\n",
    "new_word_features = [word for (word,count) in new_word_items]\n",
    "print(new_word_features[:30])\n",
    "\n",
    "# now re-run one of the feature set definitions with the new_word_features instead of word_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "157\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['.', ',', 'film', 'movie', 'not', 'one', 'like', '\"', '--', 'story', 'no', 'much', 'even', 'good', 'comedy', 'time', 'characters', 'little', 'way', 'funny', 'make', 'enough', 'never', 'makes', 'may', 'us', 'work', 'best', 'bad', 'director']\n"
     ]
    }
   ],
   "source": [
    "# Let’s try using a stopword list to prune the word features. We’ll start with the NLTK stop word list, but we’ll remove some of the negation words, or parts of words, that our negation filter uses. This list is still pretty large.\n",
    "\n",
    "# (In this question the python parts are preceded by the prompt >>> .)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print(len(stopwords))\n",
    "print(stopwords[:10])\n",
    "\n",
    "newstopwords = [word for word in stopwords if word not in negationwords]\n",
    "\n",
    "print(len(newstopwords))\n",
    "\n",
    "print(newstopwords[:10])\n",
    "\n",
    "# Now take the new stop words out of the collection of all words, and then take the top 2000 to be the word features.\n",
    "\n",
    "new_all_words_list = [word for word in all_words_list if word not in newstopwords]\n",
    "\n",
    "# Now continue to get new word features of length 2000 after the stopwords are removed:\n",
    "\n",
    "new_all_words = nltk.FreqDist(new_all_words_list)\n",
    "\n",
    "new_word_items = new_all_words.most_common(2000)\n",
    "\n",
    "new_word_features = [word for (word,count) in new_word_items]\n",
    "\n",
    "print(new_word_features[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "Most Informative Features\n",
      "             V_wonderful = True              pos : neg    =     21.6 : 1.0\n",
      "            V_engrossing = True              pos : neg    =     20.9 : 1.0\n",
      "                  V_dull = True              neg : pos    =     15.5 : 1.0\n",
      "               V_routine = True              neg : pos    =     15.0 : 1.0\n",
      "               V_generic = True              neg : pos    =     15.0 : 1.0\n",
      "             V_inventive = True              pos : neg    =     14.3 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "                V_boring = True              neg : pos    =     13.6 : 1.0\n",
      "                  V_flat = True              neg : pos    =     13.4 : 1.0\n",
      "             V_absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "                  V_warm = True              pos : neg    =     11.8 : 1.0\n",
      "                    V_90 = True              neg : pos    =     11.7 : 1.0\n",
      "              V_mindless = True              neg : pos    =     11.0 : 1.0\n",
      "              V_powerful = True              pos : neg    =     11.0 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     11.0 : 1.0\n",
      "                 V_vivid = True              pos : neg    =     11.0 : 1.0\n",
      "               V_quietly = True              pos : neg    =     11.0 : 1.0\n",
      "              V_provides = True              pos : neg    =     10.6 : 1.0\n",
      "                 V_stale = True              neg : pos    =     10.4 : 1.0\n",
      "              V_chilling = True              pos : neg    =     10.3 : 1.0\n",
      "                V_deftly = True              pos : neg    =     10.3 : 1.0\n",
      "              V_captures = True              pos : neg    =     10.2 : 1.0\n",
      "                    V_tv = True              neg : pos    =     10.2 : 1.0\n",
      "                V_beauty = True              pos : neg    =      9.8 : 1.0\n",
      "                 V_flaws = True              pos : neg    =      9.8 : 1.0\n",
      "            V_apparently = True              neg : pos    =      9.7 : 1.0\n",
      "            V_meandering = True              neg : pos    =      9.7 : 1.0\n",
      "                  V_ages = True              pos : neg    =      9.6 : 1.0\n",
      "             V_realistic = True              pos : neg    =      9.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# One strategy with negation words is to negate the word following the negation word\n",
    "#   other strategies negate all words up to the next punctuation\n",
    "# Strategy is to go through the document words in order adding the word features,\n",
    "#   but if the word follows a negation words, change the feature to negated word\n",
    "# Start the feature set with all 2000 word features and 2000 Not word features set to false\n",
    "def NOT_features(document, new_word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in new_word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in new_word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in new_word_features)\n",
    "    return features\n",
    "\n",
    "# define the feature sets\n",
    "NOT_featuresets = [(NOT_features(d, new_word_features, negationwords), c) for (d, c) in documents]\n",
    "# show the values of a couple of example features\n",
    "print(NOT_featuresets[0][0]['V_NOTcare'])\n",
    "print(NOT_featuresets[0][0]['V_always'])\n",
    "\n",
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
