{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2 Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated in the lab session:\n",
    "<br><br>\n",
    "Choose a file that you want to work onâ€”either one of the files from the book corpus or one from the Gutenberg corpus.\n",
    "<br><br>\n",
    "Make a bigram finder and experiment with whether to apply the filters or not. Run the scoring with both the raw frequency and the pmi scorers and compare results.\n",
    "<br><br>\n",
    "To complete the exercise, choose one of your top 20 frequency lists to report to show to the class. Write an introductory sentence or paragraph telling what text you chose and what bigram filters and scorer you used. Put this and the frequency list in your response. You may check out the frequency lists of other corpora by other students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for bigrams and bigram measures\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ramosem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/ramosem/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_filter(w):\n",
    "#pattern to match a word of non-alphabetical characters\n",
    "    pattern\t=re.compile('^[^a-z]+$')\n",
    "    if(pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare-hamlet.txt\n"
     ]
    }
   ],
   "source": [
    "file1 = nltk.corpus.gutenberg.fileids( )[15]\n",
    "print(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162881\n"
     ]
    }
   ],
   "source": [
    "emmatext1 = nltk.corpus.gutenberg.raw(file1)\n",
    "print(len(emmatext1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare-caesar.txt\n"
     ]
    }
   ],
   "source": [
    "file2 = nltk.corpus.gutenberg.fileids( )[14]\n",
    "print(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112310\n"
     ]
    }
   ],
   "source": [
    "emmatext2 = nltk.corpus.gutenberg.raw(file2)\n",
    "print(len(emmatext2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Tokens and Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book 1\n",
    "emmatokens1 = tknzr.tokenize(emmatext1)\n",
    "emmawords1 = [w.lower( ) for w in emmatokens1] \n",
    "\n",
    "nostop1 = [w for w in emmawords2 if (w not in stopwords)] \n",
    "freqwords1 = [w for w in nostop1 if (w not in string.punctuation)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book 2\n",
    "emmatokens2 = tknzr.tokenize(emmatext2)\n",
    "emmawords2 = [w.lower( ) for w in emmatokens2] \n",
    "\n",
    "nostop2 = [w for w in emmawords2 if (w not in stopwords)] \n",
    "freqwords2 = [w for w in nostop2 if (w not in string.punctuation)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a frequency distribution of words\n",
    "ndist1 = FreqDist(freqwords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caesar \t 190\n",
      "brutus \t 161\n",
      "bru \t 153\n",
      "haue \t 147\n",
      "shall \t 125\n"
     ]
    }
   ],
   "source": [
    "# print the top 30 tokens by frequency\n",
    "nitems1 = ndist1.most_common(5)\n",
    "for item in nitems1:\n",
    "    print (item[0], '\\t', item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to remove the stop words from Book1 as they have very high frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "total1 = ndist1.N()\n",
    "for word in ndist1:\n",
    "    ndist1[word] /= float(total1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caesar \t 0.017189903193703067\n"
     ]
    }
   ],
   "source": [
    "# print the top 30 tokens by frequency\n",
    "nitems1 = ndist1.most_common(50)\n",
    "nitems1_df = pd.DataFrame(nitems1, columns=['word', 'freq'])\n",
    "\n",
    "for item in nitems1:\n",
    "    print (item[0], '\\t', item[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caesar</td>\n",
       "      <td>0.017190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brutus</td>\n",
       "      <td>0.014566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bru</td>\n",
       "      <td>0.013842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>haue</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shall</td>\n",
       "      <td>0.011309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word      freq\n",
       "0  caesar  0.017190\n",
       "1  brutus  0.014566\n",
       "2     bru  0.013842\n",
       "3    haue  0.013300\n",
       "4   shall  0.011309"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nitems1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a frequency distribution of words\n",
    "ndist2 = FreqDist(freqwords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caesar \t 190\n",
      "brutus \t 161\n",
      "bru \t 153\n",
      "haue \t 147\n",
      "shall \t 125\n"
     ]
    }
   ],
   "source": [
    "# print the top 30 tokens by frequency\n",
    "nitems2 = ndist2.most_common(5)\n",
    "for item in nitems2:\n",
    "    print (item[0], '\\t', item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "total2 = ndist2.N()\n",
    "for word in ndist2:\n",
    "    ndist2[word] /= float(total2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caesar \t 0.017189903193703067\n",
      "brutus \t 0.014566181127295756\n",
      "bru \t 0.013842395729666154\n",
      "haue \t 0.013299556681443952\n",
      "shall \t 0.011309146837962544\n"
     ]
    }
   ],
   "source": [
    "# print the top 30 tokens by frequency\n",
    "nitems2 = ndist2.most_common(50)\n",
    "nitems1_df = pd.DataFrame(nitems1, columns=['word', 'freq'])\n",
    "\n",
    "for item in nitems2:\n",
    "    print (item[0], '\\t', item[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams and Bigram frequency distribution\n",
    "emmabigrams1 = list(nltk.bigrams(emmawords1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'the', 'tragedie', 'of', 'hamlet']\n",
      "[('[', 'the'), ('the', 'tragedie'), ('tragedie', 'of'), ('of', 'hamlet'), ('hamlet', 'by')]\n"
     ]
    }
   ],
   "source": [
    "print(emmawords1[:5])\n",
    "print(emmabigrams1[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams and Bigram frequency distribution\n",
    "emmabigrams2 = list(nltk.bigrams(emmawords2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'the', 'tragedie', 'of', 'julius']\n",
      "[('[', 'the'), ('the', 'tragedie'), ('tragedie', 'of'), ('of', 'julius'), ('julius', 'caesar')]\n"
     ]
    }
   ],
   "source": [
    "print(emmawords2[:5])\n",
    "print(emmabigrams2[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create FInders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('my', 'lord'), 0.004863002278663925)\n",
      "(('in', 'the'), 0.0020285666648140943)\n",
      "(('to', 'the'), 0.0016673150669704886)\n",
      "(('of', 'the'), 0.0016395264825209803)\n",
      "(('i', 'haue'), 0.0014727949758239316)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder1_raw = BigramCollocationFinder.from_words(emmawords1)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder1_raw.apply_word_filter(alpha_filter)\n",
    "\n",
    "scored1_raw = finder1_raw.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "for bscore in scored1_raw[:5]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many the's.. Lets get rid of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('lord', 'ham'), 0.0012226977157783583)\n",
      "(('enter', 'king'), 0.0003334630133940977)\n",
      "(('enter', 'hamlet'), 0.0002778858444950814)\n",
      "(('haue', 'seene'), 0.0002778858444950814)\n",
      "(('lord', 'hamlet'), 0.0002778858444950814)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder1_raw = BigramCollocationFinder.from_words(emmawords1)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder1_raw.apply_word_filter(alpha_filter)\n",
    "finder1_raw.apply_word_filter(lambda w: w in stopwords)\n",
    "\n",
    "scored1_raw = finder1_raw.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "for bscore in scored1_raw[:5]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to get words longer than two letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('lord', 'ham'), 0.0012226977157783583)\n",
      "(('enter', 'king'), 0.0003334630133940977)\n",
      "(('enter', 'hamlet'), 0.0002778858444950814)\n",
      "(('haue', 'seene'), 0.0002778858444950814)\n",
      "(('lord', 'hamlet'), 0.0002778858444950814)\n",
      "(('good', 'lord'), 0.0002500972600455733)\n",
      "(('thou', 'hast'), 0.0002500972600455733)\n",
      "(('enter', 'polonius'), 0.00022230867559606515)\n",
      "(('fathers', 'death'), 0.00022230867559606515)\n",
      "(('lord', 'polon'), 0.00022230867559606515)\n",
      "(('dost', 'thou'), 0.000194520091146557)\n",
      "(('good', 'friends'), 0.000194520091146557)\n",
      "(('haue', 'heard'), 0.00016673150669704886)\n",
      "(('mine', 'owne'), 0.00016673150669704886)\n",
      "(('thou', 'art'), 0.00016673150669704886)\n",
      "(('enter', 'horatio'), 0.0001389429222475407)\n",
      "(('hamlet', 'ham'), 0.0001389429222475407)\n",
      "(('tell', 'vs'), 0.0001389429222475407)\n",
      "(('wilt', 'thou'), 0.0001389429222475407)\n",
      "(('would', 'haue'), 0.0001389429222475407)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder1_raw = BigramCollocationFinder.from_words(emmawords1)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder1_raw.apply_word_filter(alpha_filter)\n",
    "finder1_raw.apply_word_filter(lambda w: w in stopwords)\n",
    "finder1_raw.apply_ngram_filter(lambda w1, w2: len(w1) < 4)\n",
    "\n",
    "scored1_raw = finder1_raw.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "for bscore in scored1_raw[:20]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information (Min Freq 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('any', 'thing'), 8.72575719277427)\n",
      "(('fathers', 'death'), 8.68393701707964)\n",
      "(('set', 'downe'), 8.297204887020944)\n",
      "(('our', 'selues'), 8.112780315883514)\n",
      "(('dost', 'thou'), 7.768825914666154)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder1_pmi = BigramCollocationFinder.from_words(emmawords1)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder1_pmi.apply_word_filter(alpha_filter)\n",
    "finder1_pmi.apply_freq_filter(5)\n",
    "# finder1_pmi.apply_word_filter(lambda w: w in stopwords)\n",
    "# finder1_pmi.apply_ngram_filter(lambda w1, w2: len(w1) < 4)\n",
    "\n",
    "scored1_pmi = finder1_pmi.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "for bscore in scored1_pmi[:5]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of small words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('fathers', 'death'), 8.68393701707964)\n",
      "(('dost', 'thou'), 7.768825914666154)\n",
      "(('wilt', 'thou'), 7.74283070613321)\n",
      "(('enter', 'polonius'), 7.403829097886904)\n",
      "(('mine', 'owne'), 7.306482700608953)\n",
      "(('your', 'lordship'), 7.15215455421766)\n",
      "(('good', 'friends'), 7.079865693410779)\n",
      "(('thou', 'art'), 7.005865111967003)\n",
      "(('thou', 'hast'), 6.835940110524692)\n",
      "(('looke', 'where'), 6.77495945884926)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder1_pmi = BigramCollocationFinder.from_words(emmawords1)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder1_pmi.apply_word_filter(alpha_filter)\n",
    "finder1_pmi.apply_freq_filter(5)\n",
    "# finder1_pmi.apply_word_filter(lambda w: w in stopwords)\n",
    "finder1_pmi.apply_ngram_filter(lambda w1, w2: len(w1) < 4)\n",
    "\n",
    "scored1_pmi = finder1_pmi.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "for bscore in scored1_pmi[:10]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too much thou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('fathers', 'death'), 8.68393701707964)\n",
      "(('enter', 'polonius'), 7.403829097886904)\n",
      "(('mine', 'owne'), 7.306482700608953)\n",
      "(('good', 'friends'), 7.079865693410779)\n",
      "(('haue', 'seene'), 6.683937017079643)\n",
      "(('haue', 'heard'), 6.461544595743193)\n",
      "(('tell', 'vs'), 6.076615158710361)\n",
      "(('enter', 'horatio'), 5.725757192774269)\n",
      "(('enter', 'hamlet'), 5.403829097886906)\n",
      "(('lord', 'polon'), 5.166121426761199)\n",
      "(('enter', 'king'), 4.892867178609524)\n",
      "(('lord', 'ham'), 4.476875777660222)\n",
      "(('lord', 'hamlet'), 4.0921208453174245)\n",
      "(('good', 'lord'), 3.969264097531891)\n",
      "(('would', 'haue'), 3.9184022707166655)\n",
      "(('hamlet', 'ham'), 2.4166152528427496)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder1_pmi = BigramCollocationFinder.from_words(emmawords1)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder1_pmi.apply_word_filter(alpha_filter)\n",
    "finder1_pmi.apply_freq_filter(5)\n",
    "finder1_pmi.apply_word_filter(lambda w: w in stopwords)\n",
    "finder1_pmi.apply_ngram_filter(lambda w1, w2: len(w1) < 4)\n",
    "finder1_pmi.apply_ngram_filter(lambda w1, w2: ((w1 == 'thou') | (w2 == 'thou')))\n",
    "\n",
    "scored1_pmi = finder1_pmi.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "for bscore in scored1_pmi[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('i', 'will'), 0.0020004801152276545)\n",
      "(('i', 'am'), 0.0019204609106185484)\n",
      "(('in', 'the'), 0.0016003840921821237)\n",
      "(('my', 'lord'), 0.0016003840921821237)\n",
      "(('it', 'is'), 0.0014803552852684645)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder2_raw = BigramCollocationFinder.from_words(emmawords2)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder2_raw.apply_word_filter(alpha_filter)\n",
    "\n",
    "scored2_raw = finder2_raw.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "for bscore in scored2_raw[:5]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('let', 'vs'), 0.0006401536368728495)\n",
      "(('mark', 'antony'), 0.0005201248299591902)\n",
      "(('marke', 'antony'), 0.0004801152276546371)\n",
      "(('thou', 'art'), 0.00044010562535008404)\n",
      "(('art', 'thou'), 0.00036008642074097783)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder2_raw = BigramCollocationFinder.from_words(emmawords2)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder2_raw.apply_word_filter(alpha_filter)\n",
    "finder2_raw.apply_word_filter(lambda w: w in stopwords)\n",
    "\n",
    "scored2_raw = finder2_raw.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "for bscore in scored2_raw[:5]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to get words longer than two letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('mark', 'antony'), 0.0005201248299591902)\n",
      "(('marke', 'antony'), 0.0004801152276546371)\n",
      "(('thou', 'art'), 0.00044010562535008404)\n",
      "(('enter', 'brutus'), 0.00036008642074097783)\n",
      "(('noble', 'brutus'), 0.00036008642074097783)\n",
      "(('thou', 'hast'), 0.00036008642074097783)\n",
      "(('caesar', 'caes'), 0.00032007681843642473)\n",
      "(('good', 'morrow'), 0.00032007681843642473)\n",
      "(('good', 'night'), 0.00032007681843642473)\n",
      "(('haue', 'done'), 0.00032007681843642473)\n",
      "(('lord', 'bru'), 0.00032007681843642473)\n",
      "(('antony', 'ant'), 0.0002800672161318716)\n",
      "(('enter', 'lucius'), 0.0002800672161318716)\n",
      "(('come', 'downe'), 0.00024005761382731855)\n",
      "(('euery', 'man'), 0.00024005761382731855)\n",
      "(('haue', 'seene'), 0.00024005761382731855)\n",
      "(('would', 'haue'), 0.00024005761382731855)\n",
      "(('caesar', 'shall'), 0.00020004801152276547)\n",
      "(('caius', 'cassius'), 0.00020004801152276547)\n",
      "(('caius', 'ligarius'), 0.00020004801152276547)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder2_raw = BigramCollocationFinder.from_words(emmawords2)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder2_raw.apply_word_filter(alpha_filter)\n",
    "finder2_raw.apply_word_filter(lambda w: w in stopwords)\n",
    "finder2_raw.apply_ngram_filter(lambda w1, w2: len(w1) < 4)\n",
    "# finder2_pmi.apply_freq_filter(5)\n",
    "\n",
    "scored2_raw = finder2_raw.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "for bscore in scored2_raw[:20]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information (Min Freq 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('any', 'thing'), 8.72575719277427)\n",
      "(('fathers', 'death'), 8.68393701707964)\n",
      "(('set', 'downe'), 8.297204887020944)\n",
      "(('our', 'selues'), 8.112780315883514)\n",
      "(('dost', 'thou'), 7.768825914666154)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder2_pmi = BigramCollocationFinder.from_words(emmawords2)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder2_pmi.apply_word_filter(alpha_filter)\n",
    "finder2_pmi.apply_freq_filter(5)\n",
    "# finder2_pmi.apply_word_filter(lambda w: w in stopwords)\n",
    "# finder2_pmi.apply_ngram_filter(lambda w1, w2: len(w1) < 4)\n",
    "\n",
    "scored2_pmi = finder2_pmi.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "for bscore in scored2_pmi[:5]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of small words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('caius', 'ligarius'), 10.060857561374691)\n",
      "(('metellus', 'cymber'), 10.060857561374691)\n",
      "(('mine', 'owne'), 9.07947323954204)\n",
      "(('fell', 'downe'), 8.822697824179928)\n",
      "(('mark', 'antony'), 8.380475495574851)\n",
      "(('marke', 'antony'), 7.643509901408645)\n",
      "(('good', 'morrow'), 7.573120573517249)\n",
      "(('most', 'noble'), 7.516537045150882)\n",
      "(('what', 'trade'), 7.368979856737022)\n",
      "(('thou', 'hast'), 7.126374214511063)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder2_pmi = BigramCollocationFinder.from_words(emmawords2)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder2_pmi.apply_word_filter(alpha_filter)\n",
    "finder2_pmi.apply_freq_filter(5)\n",
    "# finder2_pmi.apply_word_filter(lambda w: w in stopwords)\n",
    "finder2_pmi.apply_ngram_filter(lambda w1, w2: len(w1) < 4)\n",
    "\n",
    "scored2_pmi = finder2_pmi.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "for bscore in scored2_pmi[:10]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('caius', 'ligarius'), 10.060857561374691)\n",
      "(('metellus', 'cymber'), 10.060857561374691)\n",
      "(('mine', 'owne'), 9.07947323954204)\n",
      "(('fell', 'downe'), 8.822697824179928)\n",
      "(('mark', 'antony'), 8.380475495574851)\n",
      "(('marke', 'antony'), 7.643509901408645)\n",
      "(('good', 'morrow'), 7.573120573517249)\n",
      "(('honourable', 'men'), 7.123867358900492)\n",
      "(('haue', 'seene'), 6.994584341955523)\n",
      "(('haue', 'beene'), 6.924195014064127)\n",
      "(('caius', 'cassius'), 6.821391626679302)\n",
      "(('enter', 'lucius'), 6.7162093899872435)\n",
      "(('euery', 'man'), 6.605542051209625)\n",
      "(('haue', 'heard'), 6.561624934679418)\n",
      "(('come', 'downe'), 6.461241365105925)\n",
      "(('good', 'night'), 6.232083655682182)\n",
      "(('haue', 'done'), 5.886059885177357)\n",
      "(('shall', 'finde'), 5.87797515504567)\n",
      "(('antony', 'ant'), 5.602867916911299)\n",
      "(('decius', 'brutus'), 5.512842561593139)\n",
      "(('noble', 'brutus'), 5.162900090536178)\n",
      "(('lord', 'bru'), 4.892474724740785)\n",
      "(('caesar', 'caes'), 4.7540363588775385)\n",
      "(('enter', 'antony'), 4.702403590462215)\n",
      "(('great', 'caesar'), 4.6609269544860545)\n",
      "(('would', 'haue'), 4.535152723318227)\n",
      "(('enter', 'brutus'), 4.448302309398425)\n",
      "(('shall', 'haue'), 2.7657656514596454)\n",
      "(('caesar', 'shall'), 2.39558238796506)\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder and score the bigrams by frequency\n",
    "finder2_pmi = BigramCollocationFinder.from_words(emmawords2)\n",
    "\n",
    "# apply a filter to remove non-alphabetical tokens from the emma bigram finder\n",
    "finder2_pmi.apply_word_filter(alpha_filter)\n",
    "finder2_pmi.apply_freq_filter(5)\n",
    "finder2_pmi.apply_word_filter(lambda w: w in stopwords)\n",
    "finder2_pmi.apply_ngram_filter(lambda w1, w2: len(w1) < 4)\n",
    "finder2_pmi.apply_ngram_filter(lambda w1, w2: ((w1 == 'thou') | (w2 == 'thou')))\n",
    "\n",
    "scored2_pmi = finder2_pmi.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "for bscore in scored2_pmi[:50]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
